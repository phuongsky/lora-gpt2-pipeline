project/
â”œâ”€â”€ config.yaml                  # âœ… Cáº¥u hÃ¬nh pipeline (dataset, model, training, LoRA)
â”œâ”€â”€ train.py                     # âœ… Script train chÃ­nh
â”œâ”€â”€ infer_api.py                 # âœ… FastAPI Inference Server
â”œâ”€â”€ utils.py                     # âœ… Helper (load config, tokenizer, etc.)
â””â”€â”€ requirements.txt             # âœ… CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
# Huáº¥n luyá»‡n
python train.py
Cháº¡y UI:
python app_ui.py
# Cháº¡y inference API
uvicorn infer_api:app --reload --port 8000
POST http://localhost:8000/generate
{
  "prompt": "Once upon a time",
  "max_new_tokens": 50,
  "temperature": 0.8
}
lora-nlp-project/
â”œâ”€â”€ config.yaml                   # YAML cáº¥u hÃ¬nh toÃ n bá»™ pipeline
â”œâ”€â”€ train.py                      # Training script chÃ­nh (multi-GPU)
â”œâ”€â”€ utils.py                      # Helper: load config, tokenizer, model
â”œâ”€â”€ infer_api.py                  # FastAPI inference server
â”œâ”€â”€ app_ui.py                     # Gradio / Streamlit UI
â”œâ”€â”€ Dockerfile                    # Docker hÃ³a inference API
â”œâ”€â”€ requirements.txt              # Requirements cho training + inference
â”œâ”€â”€ README.md                     # HÆ°á»›ng dáº«n
â””â”€â”€ .gitignore
Build Docker image:
docker build -t lora-infer .
Cháº¡y inference API:
docker run -p 8000:8000 lora-infer
âš¡ 4. Huáº¥n luyá»‡n Multi-GPU + DeepSpeed + WandB
Cháº¡y huáº¥n luyá»‡n multi-GPU:
accelerate launch --multi_gpu train.py
Hoáº·c vá»›i deepspeed:
deepspeed train.py

# ğŸ¤– LoRA NLP Pipeline with Quantization, Gradio UI & Inference API

This project provides a complete end-to-end solution for:
- Fine-tuning `DistilGPT2` with ğŸ§  **LoRA** (via `peft`)
- Efficient training with ğŸ”¥ **DeepSpeed + quantization (4-bit/8-bit)**
- Deployment-ready **FastAPI** server + **Gradio UI**
- YAML-based config for reproducibility
- Optional W&B logging

## ğŸš€ Features

- âœ… Train with LoRA & HuggingFace PEFT
- âœ… Multi-dataset & multi-GPU training
- âœ… Quantized inference with bitsandbytes
- âœ… Push adapter to Hugging Face Hub
- âœ… Docker container for API deployment
- âœ… Gradio UI for demo

## ğŸ› ï¸ Quickstart

```bash
# 1. Install deps
pip install -r requirements.txt

# 2. Train model
python train.py

# 3. Run API
uvicorn infer_api:app --port 8000

# 4. Run UI
python app_ui.py
----------------------------

# LoRA GPTâ€‘2 Pipeline: Train, Quantize, Serve, UI & Docker

##  Features
- Fineâ€‘tune DistilGPTâ€‘2 with LoRA + 4â€‘bit Quantization
- Multiâ€‘GPU training with DeepSpeed
- Logging via Weights & Biases (W&B)
- Inference via FastAPI + Gradio UI
- Docker container for easy deployment
- YAMLâ€‘based configuration for reproducibility
- Push adapter + tokenizer to Hugging Face Hub

##  Setup & Usage

###  Clone & install
```bash
git clone https://github.com/your-username/lora-gpt2-pipeline.git
cd lora-gpt2-pipeline
pip install -r requirements.txt
